{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "from string import ascii_letters \n",
    "from unidecode import unidecode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Portuguese': tensor([0]),\n",
       " 'Dutch': tensor([1]),\n",
       " 'Polish': tensor([2]),\n",
       " 'Czech': tensor([3]),\n",
       " 'Irish': tensor([4]),\n",
       " 'Italian': tensor([5]),\n",
       " 'Spanish': tensor([6]),\n",
       " 'Japanese': tensor([7]),\n",
       " 'English': tensor([8]),\n",
       " 'Korean': tensor([9]),\n",
       " 'French': tensor([10]),\n",
       " 'Scottish': tensor([11]),\n",
       " 'Chinese': tensor([12]),\n",
       " 'Arabic': tensor([13]),\n",
       " 'Russian': tensor([14]),\n",
       " 'German': tensor([15]),\n",
       " 'Greek': tensor([16]),\n",
       " 'Vietnamese': tensor([17])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/names\"\n",
    "\n",
    "lang2label = {\n",
    "    file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)\n",
    "    for i, file_name in enumerate(os.listdir(data_dir))\n",
    "}\n",
    "lang2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_langs = len(lang2label)\n",
    "num_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slusarski'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"Ślusàrski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, ' ': 52, '.': 53, ',': 54, ':': 55, ';': 56, '-': 57, \"'\": 58}\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx); \n",
    "print(char2idx) \n",
    "print(num_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2tensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, num_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 59])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "data = name2tensor(\"abc\")\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f]\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError as e:\n",
    "                pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(target_langs)), \n",
    "    test_size=0.1, \n",
    "    shuffle=True, \n",
    "    stratify=target_langs\n",
    ")\n",
    "\n",
    "train_dataset = [\n",
    "    (tensor_names[i], target_langs[i])\n",
    "    for i in train_idx\n",
    "]\n",
    "\n",
    "test_dataset = [\n",
    "    (tensor_names[i], target_langs[i])\n",
    "    for i in test_idx\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18063\n",
      "Test: 2007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyRNN(num_letters, hidden_size, num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [3000/18063], Loss: 3.4199\n",
      "Epoch [1/2], Step [6000/18063], Loss: 1.2014\n",
      "Epoch [1/2], Step [9000/18063], Loss: 0.4216\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.0047\n",
      "Epoch [1/2], Step [15000/18063], Loss: 0.3698\n",
      "Epoch [1/2], Step [18000/18063], Loss: 0.5171\n",
      "Epoch [2/2], Step [3000/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.3970\n",
      "Epoch [2/2], Step [9000/18063], Loss: 1.1348\n",
      "Epoch [2/2], Step [12000/18063], Loss: 4.4956\n",
      "Epoch [2/2], Step [15000/18063], Loss: 0.3938\n",
      "Epoch [2/2], Step [18000/18063], Loss: 0.4349\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.6423%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2lang = {label.item(): lang for lang, label in lang2label.items()}\n",
    "\n",
    "def myrnn_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in tensor_name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()    \n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Mike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Japanese'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Akutagawa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
